{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation & Topic Models\n",
    "\n",
    "In this notebook, we describe the basics of LDA for learning topics of text corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Trivial\" example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a trivial data set consisting of just a few very short \"documents\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human': 0, 'interface': 1, 'computer': 2, 'user': 3, 'system': 4, 'response': 5, 'time': 6, 'eps': 7, 'survey': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "# The documents themselves:\n",
    "docs    = ['Human machine interface for ABC computer applications',\n",
    "           'A survey of user opinion of computer system response time',\n",
    "           'The EPS user interface management system',\n",
    "           'System and human system engineering testing of EPS',\n",
    "           'Relation of user perceived response time to error measurement',\n",
    "           'The generation of random, binary, ordered trees',\n",
    "           'The intersection graph of paths in trees',\n",
    "           'Graph minors IV: Widths of trees and well-quasi-ordering',\n",
    "           'Graph minors: A survey']\n",
    "\n",
    "# Normally, we would remove stop words, etc; \n",
    "# here we'll just use a pre-constructed \"interesting word\" list\n",
    "vocab_list = ['human','interface','computer','user','system','response','time', \n",
    "         'EPS','survey','trees','graph','minors']\n",
    "\n",
    "vocab = {w.lower():i for i,w in enumerate(vocab_list)}\n",
    "words = {i:w.lower() for i,w in enumerate(vocab_list)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to define some text helper functions to filter the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['will', 'filter', 'out', 'all', 'unwanted', 'words']\n"
     ]
    }
   ],
   "source": [
    "stopwords = 'the and was for that you with have are this from can which has were don'.split(' ')\n",
    "# if exist stopwords.txt: load & append to stopwords\n",
    "def filter_text(text):\n",
    "    import re\n",
    "    words = re.sub(r'[^\\w ]',' ',text)\n",
    "    words = map(lambda w: w.lower(), words.split(' '))\n",
    "    words = filter(lambda w: w not in stopwords and len(w)>2, words)\n",
    "    return words\n",
    "    \n",
    "print(list(filter_text('This will filter out all the unwanted words, eh?')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we translate the data into index form.  Each word in the corpus is one data point, where data point i is represented by a document ID \"d[i]\" indicating which document it came from, and a word ID \"w[i]\" indicating which word in the vocabulary it was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([vocab[j] for i,txt in enumerate(docs) for j in filter_text(txt) if j in vocab ],dtype=int)\n",
    "d = np.array([i for i,txt in enumerate(docs) for j in filter_text(txt) if j in vocab ],dtype=int)\n",
    "D = max(d)+1\n",
    "W = max(w)+1\n",
    "N = len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  8,  3,  2,  4,  5,  6,  7,  3,  1,  4,  4,  0,  4,  7,\n",
       "        3,  5,  6,  9, 10,  9, 10, 11,  9, 10, 11,  8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 6,\n",
       "       6, 7, 7, 7, 8, 8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs sampling inference\n",
    "The model keeps track of the frequency that words in document d were assigned to topic t, \"a[d,t]\", and that vocabulary word w was assigned to topic t, \"b[w,t]\".  For convenience we also track \"c[t]\", the total number of words assigned to topic t.\n",
    "\n",
    "We initialize the assignment vector \"z[i]\" to assign each word to a topic at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "T = 2           # Only two topics (\"HCI\" and \"Graph theory\")\n",
    "beta = .001     # LDA parameters: control sparsity of documents & topics\n",
    "alpha = .001\n",
    "\n",
    "a = np.zeros((D,T),dtype=int)   # total times a token in document d has been assigned to topic t\n",
    "b = np.zeros((W,T),dtype=int)   # total times word w has been assigned to topic t\n",
    "c = np.zeros((T,),dtype=int)    # total assignments to topic t\n",
    "\n",
    "z = np.random.randint(T,size=N);  # assign every token to a topic at random\n",
    "for n in range(N):\n",
    "    a[d[n],z[n]] += 1             # and each document ID\n",
    "    b[w[n],z[n]] += 1             # count up number for each word ID\n",
    "    c[z[n]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can run \"collapsed\" Gibbs sampling for the LDA model, which samples a value for each z[i] in turn, holding the others fixed.  In practice, we update the sufficient statistics a,b,c to remove z[i]'s assignment, use them to compute p(z[i] | z[not i]), and then re-update a,b,c to reflect the new assignment z[i]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs(maxIter):\n",
    "    for it in range(maxIter):             # for each iteration \n",
    "        for i in range(N):                # run through all the words & sample z[i]:\n",
    "            t = z[i];\n",
    "            a[d[i],t] -= 1                # remove token i's assignment from our count vectors\n",
    "            b[w[i],t] -= 1            \n",
    "            c[t]      -= 1\n",
    "\n",
    "            # Compute topic probability distribution given current counts\n",
    "            probs = (beta + b[w[i],:])/(c[:] + beta*W) * (alpha + a[d[i],:])\n",
    "\n",
    "            # Now, normalize and draw a sample from the distribution over topics:\n",
    "            cumprobs = np.cumsum(probs); cumprobs /= cumprobs[-1] \n",
    "            t = np.where(cumprobs>np.random.rand())[0][0]\n",
    "\n",
    "            z[i] = t;                      # now add this assignment back into our count vectors\n",
    "            a[d[i],t] += 1\n",
    "            b[w[i],t] += 1\n",
    "            c[t]      += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for visualizing the learning process\n",
    "def print_topics():\n",
    "    '''Print the top 8 words in each topic given the current assignments'''\n",
    "    for t in range(T):\n",
    "        isort = np.argsort(-b[:,t])  # find the most likely words for topic t\n",
    "        xsort = b[isort,t]           # then print topic, % tokens explained, & top 8 words\n",
    "        print('[{}] ({:.3f}) {}'.format(t, 1.*c[t]/N, list(words[isort[ww]] for ww in range(8))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's just step through a few iterations of the Gibbs sampling inference process and see what the topics look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] (0.414) ['human', 'system', 'interface', 'user', 'response', 'eps', 'survey', 'trees']\n",
      "[1] (0.586) ['computer', 'user', 'system', 'time', 'trees', 'graph', 'interface', 'response']\n"
     ]
    }
   ],
   "source": [
    "print_topics();       # before running any iterations: random assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice at this point the topics are \"blended\", with some words from the HCI papers and some from the graph papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] (0.483) ['system', 'human', 'interface', 'computer', 'user', 'eps', 'response', 'time']\n",
      "[1] (0.517) ['trees', 'graph', 'response', 'time', 'survey', 'minors', 'user', 'human']\n"
     ]
    }
   ],
   "source": [
    "gibbs(20)             # run a few iterations\n",
    "print_topics();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] (0.690) ['system', 'user', 'human', 'interface', 'computer', 'response', 'time', 'eps']\n",
      "[1] (0.310) ['trees', 'graph', 'minors', 'survey', 'human', 'interface', 'computer', 'user']\n"
     ]
    }
   ],
   "source": [
    "gibbs(180)            # run a bunch of iterations\n",
    "print_topics();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we see that the HCI topic is more common (there are more documents that use it), and the words are now associated with each other in a more coherent way.\n",
    "\n",
    "We can also visualize what's happening by looking at how many words are assigned to each topic in the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0]: human machine interface abc computer applications\n",
      "[6 0]: survey user opinion computer system response time\n",
      "[4 0]: eps user interface management system\n",
      "[4 0]: system human system engineering testing eps\n",
      "[3 0]: relation user perceived response time error measurement\n",
      "[0 1]: generation random binary ordered trees\n",
      "[0 2]: intersection graph paths trees\n",
      "[0 3]: graph minors widths trees well quasi ordering\n",
      "[0 3]: graph minors survey\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(a)): print(str(a[i])+\": \"+\" \".join(filter_text(docs[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first 5 documents are all assigned to topic 1; the last 4 all to topic 2.  We see a similar story for the vocabulary words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0]: human\n",
      "[2 0]: interface\n",
      "[2 0]: computer\n",
      "[3 0]: user\n",
      "[4 0]: system\n",
      "[2 0]: response\n",
      "[2 0]: time\n",
      "[2 0]: eps\n",
      "[1 1]: survey\n",
      "[0 3]: trees\n",
      "[0 3]: graph\n",
      "[0 2]: minors\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(b)): print(str(b[j])+\": \"+words[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most vocabulary words are exclusively assigned to the topic most associated with their documents, although the word \"survey\" appears in both types of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Real data\n",
    "Now we can look at how to do this on some real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build vocab list (if necessary)\n",
    "min_word_count = 5      # minimum length for an non-stopword\n",
    "D,W,N = 0,0,0\n",
    "\n",
    "# Data set is a few NYTimes articles from Jan 1 2000:\n",
    "corpus = '../../../ml-data/topicmodel/example1/*.txt'\n",
    "\n",
    "# run over documents, collecting words that occur:\n",
    "vocab = {}\n",
    "import glob\n",
    "for filename in glob.glob(corpus):\n",
    "    with open(filename,'rt') as fh:\n",
    "        text = filter_text(fh.read())\n",
    "        for word in text:\n",
    "            if word in vocab: vocab[word] = vocab[word]+1\n",
    "            else: vocab[word] = 1\n",
    "            N += 1                 # count number of tokens (words in documents)\n",
    "    D += 1                         # count # of documents\n",
    "\n",
    "# now remove rare vocabulary:\n",
    "remove = [w for w in vocab if vocab[w] <= min_word_count]\n",
    "for w in remove: del vocab[w]\n",
    "\n",
    "# now make vocab map to a unique index, rather than a count:\n",
    "vocab = {w:i for i,w in enumerate(vocab)}   # map words to index\n",
    "words = {vocab[w]:w for w in vocab}         # and an inverse map\n",
    "W = len(vocab)   # count # of words\n",
    "\n",
    "w,d = np.zeros((N,),dtype=int),np.zeros((N,),dtype=int)   # allocate storage for data\n",
    "i = 0    \n",
    "for j,filename in enumerate(glob.glob(corpus)):\n",
    "    with open(filename,'rt') as fh:\n",
    "        text = filter_text(fh.read())\n",
    "        for word in text: \n",
    "            if word in vocab:\n",
    "                d[i] = j\n",
    "                w[i] = vocab[word]\n",
    "                i += 1\n",
    "\n",
    "N = i+1\n",
    "w = w[:N]  # get rid of extra storage\n",
    "d = d[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "T = 8           # Let's use 8 topics now\n",
    "beta = .01      # LDA parameters: control sparsity of documents & topics\n",
    "alpha = .1\n",
    "\n",
    "a = np.zeros((D,T),dtype=int)   # total times a token in document d has been assigned to topic t\n",
    "b = np.zeros((W,T),dtype=int)   # total times word w has been assigned to topic t\n",
    "c = np.zeros((T,),dtype=int)    # total assignments to topic t\n",
    "\n",
    "z = np.random.randint(T,size=N);  # assign every token to a topic at random\n",
    "for n in range(N):\n",
    "    a[d[n],z[n]] += 1             # and each document ID\n",
    "    b[w[n],z[n]] += 1             # count up number for each word ID\n",
    "    c[z[n]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] (0.124) ['his', 'said', 'but', 'new', 'they', 'year', 'who', 'had']\n",
      "[1] (0.124) ['said', 'but', 'his', 'they', 'new', 'year', 'who', 'not']\n",
      "[2] (0.125) ['said', 'his', 'new', 'but', 'they', 'who', 'not', 'year']\n",
      "[3] (0.126) ['his', 'said', 'but', 'new', 'they', 'who', 'not', 'had']\n",
      "[4] (0.127) ['said', 'his', 'new', 'but', 'not', 'who', 'year', 'they']\n",
      "[5] (0.125) ['said', 'his', 'new', 'but', 'they', 'not', 'had', 'year']\n",
      "[6] (0.125) ['said', 'his', 'new', 'who', 'but', 'they', 'year', 'not']\n",
      "[7] (0.124) ['his', 'said', 'new', 'but', 'year', 'who', 'had', 'not']\n"
     ]
    }
   ],
   "source": [
    "print_topics();       # before running any iterations: random assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that initially, the topics' word distributions are all about the same, with the most common words being most probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] (0.106) ['about', 'their', 'century', 'how', '000', 'who', 'war', 'work']\n",
      "[1] (0.116) ['who', 'but', 'many', 'people', 'there', 'they', 'into', 'percent']\n",
      "[2] (0.131) ['more', 'but', 'not', 'one', 'their', 'will', 'then', 'they']\n",
      "[3] (0.110) ['said', 'y2k', 'new', '2000', 'problems', 'system', 'other', 'states']\n",
      "[4] (0.125) ['new', 'times', 'year', 'millennium', 'york', 'nyt', 'news', 'putin']\n",
      "[5] (0.128) ['said', 'will', 'not', 'year', 'its', 'been', 'there', 'had']\n",
      "[6] (0.124) ['his', 'when', 'who', 'home', 'him', 'coach', 'been', 'sports']\n",
      "[7] (0.160) ['said', 'they', 'had', 'who', 'one', 'team', 'out', 'would']\n"
     ]
    }
   ],
   "source": [
    "gibbs(20)             # after a few iterations:\n",
    "print_topics();       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] (0.112) ['her', 'american', 'world', 'people', 'she', 'century', '000', 'life']\n",
      "[1] (0.102) ['they', 'many', 'more', 'their', 'who', 'there', 'people', 'them']\n",
      "[2] (0.190) ['his', 'will', 'one', 'new', 'who', 'out', 'like', 'more']\n",
      "[3] (0.087) ['y2k', 'said', 'new', 'year', 'problems', 'computer', 'not', 'system']\n",
      "[4] (0.118) ['new', 'times', 'year', 'millennium', 'nyt', '2000', 'york', 'news']\n",
      "[5] (0.173) ['said', 'but', 'had', 'they', 'all', 'not', 'been', 'when']\n",
      "[6] (0.084) ['his', 'when', 'him', 'been', 'about', 'sports', 'who', 'bowl']\n",
      "[7] (0.133) ['they', 'game', 'team', 'season', 'but', 'year', 'players', 'said']\n"
     ]
    }
   ],
   "source": [
    "gibbs(180)            # after a bunch of iterations\n",
    "print_topics();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the topics have resolved into something more meaningful, approximately grouped by subject area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
