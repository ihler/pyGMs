{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyGMs Introduction: Learning from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyGMs as gm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyGMs assumes discrete random variables whose values are in the range {0..d-1}, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [gm.Var(0,3), gm.Var(1,3), gm.Var(2,2), gm.Var(3,3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyGMs assumes discrete random variables whose values are in the range {0..d-1}.\n",
    "\n",
    "To represent single observations, we may use any of several useful structures.  The most common of these is a tuple of integers, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = (0,2,0,1)   # X0=0, X1=2, X2=0, X3=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, also useful representation is to use a dict that maps variables (or their integer labels) to values.  This is particularly useful when we want to represent a partial observation, i.e., when some of the variables are observed but not others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = {0:2, 3:1}  # X0=2, X3=1, other Xi are unobserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following standard convention, if we want to represent a partial observation in tuple form, we use the \"not-a-number\" float value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = (2,np.nan,np.nan,1)  # X0=2, X3=1; X1, X2 are unobserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions are not equipped to operate with missing values, in which case passing `nan` will usually cause an error.\n",
    "\n",
    "Occasionally, it is useful to represent partial observations using tuple pairs, which specify the variables that are assigned and their values, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = (X[0],X[3]);      # Xe = (0,3) (so, just the labels) also usually works\n",
    "xe = (2,1);            # (Xe,xe) represents X0=2, X3=1, other Xi unobserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple measurements (e.g., a data set), we can use a list of either the tuple or dict representations, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_dict = [ {0:2, 3:1}, {0:0, 1:3, 2:0, 3:1} ]    # data set of two points, one partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_tuple = [ (2,np.nan,np.nan,1), (0,3,0,1) ]      # same data in tuple format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some helper functions for converting from one data set representation to another, or print them out nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in dict format:\n",
      " [(2, nan, nan, 1), (0, 3, 0, 1)]\n",
      "    => [{0: 2, 3: 1}, {0: 0, 1: 3, 2: 0, 3: 1}]\n",
      "\n",
      "\n",
      "Data in tuple format:\n",
      " [{0: 2, 3: 1}, {0: 0, 1: 3, 2: 0, 3: 1}]\n",
      "    => [(2, nan, nan, 1), (0, 3, 0, 1)]\n",
      "\n",
      "\n",
      "Data in string format: ['2--1', '0301']\n"
     ]
    }
   ],
   "source": [
    "print(f'Data in dict format:\\n {D_tuple}\\n    => {gm.t2d(D_tuple)}\\n\\n')\n",
    "print(f'Data in tuple format:\\n {D_dict}\\n    => {gm.d2t(D_dict)}\\n\\n')\n",
    "print(f'Data in string format: {[gm.d2s(x) for x in D_dict]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes useful to represent the data as a numpy array, which allows some additional convenient operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2. nan nan  1.]\n",
      " [ 0.  3.  0.  1.]] \n",
      "\n",
      "Data set has [0 1 1 0] missing data in each column.\n"
     ]
    }
   ],
   "source": [
    "D = np.array([ (2,np.nan,np.nan,1), (0,3,0,1) ])\n",
    "print(D,'\\n')\n",
    "print(f'Data set has {np.isnan(D).sum(0)} missing data in each column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, numpy arrays should be used with care when interacting with factors, since they (like numpy arrays) have different behavior when indexed by a tuple than when indexed by a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing with a tuple: f[(1,2)]=6.0 gives the value at X0=1,X1=2\n",
      "Indexing with a list: f[[1,2]]=[[4. 5. 6.]\n",
      " [7. 8. 9.]] instead gives two subarrays of f\n"
     ]
    }
   ],
   "source": [
    "f = gm.Factor([X[0],X[1]], [[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(f'Indexing with a tuple: f[(1,2)]={f[(1,2)]} gives the value at X0=1,X1=2')\n",
    "print(f'Indexing with a list: f[[1,2]]={f[[1,2]]} instead gives two subarrays of f')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model Fit\n",
    "\n",
    "One of the most basic things we might like to do is evaluate whether our model \"fits\" a set of observed data.  Typically, we score this using the likelihood of the data under the model.  Let us use the Alarm Bayesian network to illustrate computing the data likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = gm.Var(0,2)   # Was there a burglary at Sherlock Holmes' house?\n",
    "E = gm.Var(1,2)   # Was there an earthquake?\n",
    "A = gm.Var(2,2)   # Has the alarm gone off?\n",
    "W = gm.Var(3,2)   # Has Watson called to complain?\n",
    "H = gm.Var(4,2)   # Has Mrs Hudson called to complain?\n",
    "\n",
    "X = [B,E,A,W,H]   # we'll often refer to variables as Xi or X[i]\n",
    "\n",
    "names = dict( (eval(n).label,n) for n in ['B','E','A','W','H'])\n",
    "\n",
    "p_E = gm.Factor([E], [.998, .002]) # probability of earthquake (false,true)\n",
    "p_B = gm.Factor([B], [.999, .001]) # probability of burglary\n",
    "p_A_BE = gm.Factor([A,B,E], 0.0)   # alarm given B,E\n",
    "# Set  B,E,A                       # \n",
    "p_A_BE[0,0,:] = [.999, .001]       # \n",
    "p_A_BE[0,1,:] = [.710, .290]       \n",
    "p_A_BE[1,0,:] = [.060, .940]       # For each setting (b,e), the table should sum to one over A\n",
    "p_A_BE[1,1,:] = [.050, .950]       #   so that it corresponds to a conditional probability\n",
    "\n",
    "p_W_A = gm.Factor([A,W], 0.0)      # Probability that Watson calls given the alarm's status\n",
    "p_W_A[0,:]    = [.95, .05]\n",
    "p_W_A[1,:]    = [.10, .90]\n",
    "\n",
    "p_H_A = gm.Factor([A,H], 0.0)      # Probability that Mrs Hudson calls given the alarm's status\n",
    "p_H_A[0,:]    = [.99, .01]\n",
    "p_H_A[1,:]    = [.30, .70]\n",
    "\n",
    "factors = [p_W_A, p_H_A, p_A_BE, p_E, p_B]  # collect all the factors that define the model\n",
    "\n",
    "model = gm.GraphModel([p_E,p_B,p_A_BE,p_W_A,p_H_A])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we observe some data, $D$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [(0, 0, 0, 1, 1), (1, 1, 0, 1, 0), (1, 1, 0, 1, 1), (1, 0, 0, 1, 0), (0, 1, 1, 1, 1), \n",
    "     (0, 1, 1, 1, 0), (0, 1, 0, 1, 1), (0, 0, 1, 1, 1), (1, 0, 1, 1, 0), (1, 0, 0, 1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How likely is this data under our model?  Since our model is normalized, we can compute the log likelihood of each point, $\\log p(x)$, by just evaluating the model at $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-7.60490546]), array([-19.12387826]), array([-23.71899811]), array([-12.72895061]), array([-7.91551841]), array([-8.76281627]), array([-14.15900137]), array([-7.37279324]), array([-8.28096601]), array([-17.32407046])]\n",
      "\n",
      "Average log-likeliood of D: -12.699189820279875\n"
     ]
    }
   ],
   "source": [
    "LL = [model.logValue(x) for x in D]\n",
    "print(LL)\n",
    "print()\n",
    "print(f'Average log-likeliood of D: {np.mean(LL)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if our model is not normalized?  In that case, we need both $\\log f(x)$, the evaluation of our model at $x$, and also the normalizing constant (or partition function), $Z$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-4.65106295]), array([-16.17003574]), array([-20.76515559]), array([-9.77510809]), array([-4.9616759]), array([-5.80897376]), array([-11.20515885]), array([-4.41895073]), array([-5.32712349]), array([-14.37022794])]\n",
      "\n",
      "Average log-likeliood of D given W=1: -9.745347304424087\n"
     ]
    }
   ],
   "source": [
    "model_W = model.copy()\n",
    "model_W.condition({W:1})  # model given that we know Watson called\n",
    "\n",
    "elim = model_W.copy()\n",
    "elim.eliminate(X, 'sum')\n",
    "lnZ = elim.logValue([])\n",
    "\n",
    "LL = [model_W.logValue(x) - lnZ for x in D]\n",
    "print(LL)\n",
    "print()\n",
    "print(f'Average log-likeliood of D given W=1: {np.mean(LL)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function `loglikelihood` can perform this calculation for you, either calculating log Z, or using a known value if optionally passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.65106295]\n",
      " [-16.17003574]\n",
      " [-20.76515559]\n",
      " [ -9.77510809]\n",
      " [ -4.9616759 ]\n",
      " [ -5.80897376]\n",
      " [-11.20515885]\n",
      " [ -4.41895073]\n",
      " [ -5.32712349]\n",
      " [-14.37022794]]\n"
     ]
    }
   ],
   "source": [
    "LL = gm.loglikelihood(model_W,D)   # pass \"logZ=lnZ\" if lnZ is already calculated\n",
    "print(LL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, a major complication of using log-likelihood for scoring models is the requirement of calculating the log partition function, which can be computationally difficult.  The computationally simpler \"pseudo-likelihood\" is sometimes used in its place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.4225763  -20.61720497 -30.64518144 -14.19056285  -1.36277531\n",
      "  -2.57890251 -17.40618398  -2.06181856  -1.94167805 -24.21651688]\n"
     ]
    }
   ],
   "source": [
    "PLL = gm.pseudologlikelihood(model_W,np.array(D))\n",
    "print(PLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not the same as likelihood, the pseudolikelihood can be used to assess relative fit or guide a learning process.  We can see that in our example samples, the log-likelihood and pseudo-loglikelihood are correlated, with data points exhibiting either relatively lower values or higher values for both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(LL,PLL,'b.',ms=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Model to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide the problem of learning a model from data into two cases: first, where the model and its structure are unknown, and second, the (easier) case in which the model's structure is known, and we only need to learn its parameter values (e.g., the conditional probabilities of a Bayesian network).\n",
    "\n",
    "**Note:** In general, pyGMs uses `fit_x` to indicate functions that operate directly from the data to select the model in full, i.e., structure and parameter learning.  We use `refit_x` to indicate functions that take an existing model structure and update its parameters (probabilities, etc.) to fit the data set, i.e., parameter learning only.\n",
    "\n",
    "Let's consider the simpler case first.  If we know the model structure, the most common approach is to select the model parameters that maximize the likelihood of our data, i.e., maximum likelihood learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical probabilities\n",
    "\n",
    "We saw in the case of a Bayesian network that the maximum likelihood solution is simply given by matching the empirical probabilities in the data. If our data are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [(1, 0, 1, 1, 0), (0, 1, 0, 1, 1), (0, 1, 1, 0, 1), (1, 1, 1, 1, 0), (0, 1, 1, 1, 1), \n",
    "     (1, 1, 1, 1, 0), (1, 1, 1, 1, 0), (0, 1, 1, 1, 0), (0, 1, 1, 0, 0), (1, 0, 1, 0, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can conveniently count the data matching various patterns using a helper function, `empirical()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical returns a list of the empirical estimates: [Factor({0},[0x153f700])]\n",
      "each of which has the empirical counts for configurations of those variables: [5. 5.]\n"
     ]
    }
   ],
   "source": [
    "clique_list = [ [X[0]] ]\n",
    "phat_list = gm.misc.empirical(clique_list, D)\n",
    "print(f'Empirical returns a list of the empirical estimates: {phat_list}')\n",
    "print(f'each of which has the empirical counts for configurations of those variables: {phat_list[0].table}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a parameter `normalize` which we can use to divide by the total number of data, e.g., resulting in estimated probabilities `[0.5, 0.5]` instead of counts `[5,5]`.\n",
    "\n",
    "If we have a list of conditional probabilities to estimate, we can compute the empirical counts for those subsets of variables, and then normalize them in a conditional manner, so that each \"row\" of the tables corresponding to the child variable sums to one. Specifically, for every possible configuration of the parent variables, we sum over the child variable and normalize using that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factors = [p_E,p_B,p_A_BE,p_W_A,p_H_A]  # to easily access the cliques we'll need\n",
    "\n",
    "phat_factors = gm.misc.empirical([f.vars for f in model_factors], D)\n",
    "\n",
    "# phat_factors is a list of each estimated counts; let's give them names:\n",
    "ph_E, ph_B, ph_ABE, ph_WA, ph_HA = phat_factors\n",
    "\n",
    "# Some of our configurations are zero, since we only have 10 data points.\n",
    "# To avoid dividing by zero, let's add a small value to ensure positivity:\n",
    "for f in [ph_E,ph_B,ph_ABE,ph_WA,ph_HA]: f+=1e-6  \n",
    "\n",
    "# Now, normalize each factor as desired:\n",
    "ph_E = ph_E/ph_E.sum()\n",
    "ph_B = ph_B/ph_B.sum()\n",
    "ph_A_BE = ph_ABE/ph_ABE.sum([A])\n",
    "ph_W_A = ph_WA/ph_WA.sum([W])\n",
    "ph_H_A = ph_HA/ph_HA.sum([H])\n",
    "\n",
    "# Now we can create a Bayes net out of these empirical conditional probabilties:\n",
    "model_estimate = gm.GraphModel([ph_E,ph_B,ph_A_BE,ph_W_A,ph_H_A])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the resulting parameters do, indeed, increase the log-likelihood of the training data. (In this case, the fact that we have very few training data to fit these parameters may indicate *overfitting*, of course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original parameters average LL: -11.327094860309254\n",
      "After setting parameters using maximum likelihood: -2.589476718797504\n"
     ]
    }
   ],
   "source": [
    "LL_original = gm.loglikelihood(model,D)   # pass \"logZ=lnZ\" if lnZ is already calculated\n",
    "print(f'Original parameters average LL: {LL_original.mean()}')\n",
    "\n",
    "LL_maxll = gm.loglikelihood(model_estimate,D)   \n",
    "print(f'After setting parameters using maximum likelihood: {LL_maxll.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Learning\n",
    "\n",
    "When we do not know the structure of the model, we need to search or select over possible structures.  It turns out this is easy to do if we would like to select the **tree** (each node has at most one parent) that results in the maximum likelihood of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chow-Liu Trees\n",
    "We can efficiently find both the tree structure and parameters that maximize the likelihood of our data over all tree-structured graphs using an algorithm known as Chow-Liu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaOklEQVR4nO3df1DT9/0H8CfqxB9BBavDTq0tMVERuSFxOo3b1btyCkPj5iY1grU7rG4OiUBqtQK1a0VqGFdbC7ObRld3u1nnid7pXDdHZ6+EKkhJJGZMp5tnwGAJij+Az/ePDr7GBATN5/MJ8Hzc+Uc/P/DlnX3e2/fn9Xl9ggRBEEBERJIYIHcBRET9CUOXiEhCDF0iIgkxdImIJMTQJSKSEEOXiEhCDF0iIgkxdImIJMTQJSKSEEOXiEhCDF0iIgkxdImIJMTQJSKSEEOXiEhCDF0iIgkNkrsAor7M6XTCbDbDbrejqakJCoUCKpUKKSkpGDNmjNzlkQy40iUSgcVigV6vh1qths1mQ0xMDOLj4xETEwOr1QqVSgW9Xg+LxSJ3qSSxIH45gsi/iouLkZ2dDaPRiJSUFISGhnpd09DQgL1792LHjh3Izc1FamqqDJWSHBi6RH5UXFyMvLw8nDhxAkql8pHXOxwOxMXFwWg0Mnj7CYYukZ9YLBYkJiaitLS0W4HbzuFwQKvV4ujRo4iNjRWxQgoE3NMl8pPCwkIYjUafgavX6zFu3DiMGDECKpUKe/bs6TinVCqRlZWFwsJCKcslmXClS+QHTqcTarUatbW1Pvdwq6uroVQqERwcjAsXLuD73/8+jh07hpkzZwIAXC4XIiIiYLfb2dXQx3GlS+QHZrMZOp3OZ+ACQGRkJIKDgwEAQUFBCAoKwj//+c+O82FhYdDpdDCbzZLUS/Jh6BL5gd1ux6xZs7q8Zt26dRg2bBimTJmCcePGYdGiRR7nNRoN7Ha7mGVSAGDoEvlBU1MTQkJCurzm/fffh9vtRmlpKZYuXdqx8m0XEhICt9stZpkUABi6RH6gUCi6FZgDBw7EvHnzcPXqVezevdvjnNvtfmRwU+/H0CXyA5VKhbKysm5f39LS4rGnC3zdcqZSqfxdGgUYdi8Q+UFX3QtOpxOffPIJEhISMHToUJw6dQpLly7FwYMHkZiYCIDdC/0JV7pEfjB27FjEx8dj3759XueCgoKwe/dujB8/HqGhocjIyMCvfvWrjsAFgH379iEhIYGB2w9wpUvkJ3wjjbqDK10iP9FoNMjNzUVcXBwcDke37mmfvZCbm8vA7ScYukR+lJqaCqPRCK1Wi4KCAjQ0NPi8zuVywWQyQavVcthNP8PtBSIRlJeXo7CwECUlJdDpdNBoNB19uBaLBYcPH0ZCQgLS0tK4wu1nGLpEIqqrq4PZbMZ7772HiIgIPPfcc1CpVEhOTuZDs36KoUskgejoaJjNZkRHR8tdCsmMe7pEEqivr8dTTz0ldxkUALjSJRKZIAgYMmQIvvrqKwwZMkTuckhmXOkSicztdmPw4MEMXALA0CUSHbcW6EEMXSKRMXTpQQxdIpHV19ezPYw6MHSJRMaVLj2IoUsksrq6OoYudWDoEomMK116EEOXSGQMXXoQQ5dIZAxdehBDl0hkDF16EEOXSGRsGaMHMXSJRMaVLj2IA2+IRNTa2org4GDcuXMHgwYNkrscCgBc6RKJqKGhASNHjmTgUgeGLpGIuLVAD2PoEomIoUsPY+gSiYidC/Qwhi6RiLjSpYcxdIlExGE39DCGLpGIuNKlhzF0iUTE0KWHMXSJRMTQpYcxdIlExNClhzF0iUTEljF6GEOXSERc6dLDOPCGSCR3795FSEgI7t69i6CgILnLoQDBlS6RSG7cuIHRo0czcMkDQ5dIJNxaIF8YukQiYeiSLwxdIpGwc4F8YegSiYQrXfKFoUskEg67IV8YukQi4UqXfGHoEomEoUu+MHSJRMLQJV8YukQiYeiSLwxdIpGwZYx8YegSiUAQBNTX12P06NFyl0IBhqFLJIJbt25hwIABGDZsmNylUIBh6BKJgPu51BmGLpEIGLrUGYYukQgYutQZhi6RCNi5QJ1h6BKJgCtd6gxDl0gEHHZDnWHoEomAK13qDEOXSAQMXeoMQ5dIBAxd6gxDl0gEDF3qDEOXSARsGaPOBAmCIMhdBFFf0tbWhuDgYNy+fRvf+MY35C6HAgxXukR+dvPmTSgUCgYu+cTQJfIz7udSVxi6RH7G0KWuMHSJ/IyhS11h6BL5GUOXusLQJfIztotRVxi6RH7GYTfUlUFyF0DUFzidTpjNZtjtdnz66acIDw/HgAEDkJKSwlUveeBKl+gJWCwW6PV6qNVq2Gw2xMTEYPPmzfjxj38Mq9UKlUoFvV4Pi8Uid6kUIPhGGtFjKi4uRnZ2NoxGI1JSUhAaGup1TUNDA/bu3YsdO3YgNzcXqampMlRKgYShS/QYiouLkZeXhxMnTkCpVD7yeofDgbi4OBiNRgZvP8ftBaIeslgsyM7O7jRwL168iCFDhkCv13ccUyqVOHHiBLKzs1FeXi5luRRgGLpEPVRYWAij0djpCvdnP/sZNBqN13GlUomsrCwUFhaKXSIFMIYuUQ84nU4cO3YMKSkpPs///ve/x6hRo7BgwQKf51NSUlBSUoK6ujoxy6QAxtAl6gGz2QydTufzoVljYyO2bt0Kk8nU6f1hYWHQ6XQwm81ilkkBjKFL1AN2ux2zZs3yee7111/Hyy+/jPHjx3f5MzQaDex2uxjlUS/AlyOIeqCpqQkhISFexysqKnDq1CmcO3fukT8jJCQEN2/eFKE66g0YukQ9oFAo4Ha7vY7/7W9/w6VLlzBx4kQAX4dza2srrFYrzp4963Gt2+3G8ePHMXXqVERHR2PGjBkdvyZMmICgoCBJ/iwkD/bpEvXAO++8A6vVit/85jcex2/fvo3GxkaP6y5duoTdu3d7vQa8evVqqNVqLFy4EOfPn+/4VVlZiebm5o4Abg/k6dOnY/jw4ZL8+Uh8DF2iHrh27RrUajUuX77s82Fau5ycHDgcDhw4cMDjuMvlQkREBOx2u8+ZDE6nE1VVVR5hbLPZ8K1vfcsrjCdNmoQBA/hYprdh6BJ101//+lcYDAbcuHED6enpSE9P7/HPKCgowNmzZ7F///5u39PS0gK73e4RxOfPn0dDQwOioqI8tidmzJiBESNG9Lgukg5Dl+gRLl68iMzMTFRWViIvLw+TJk3C4sWLUVpa2q1XgNs5HA5otVocPXoUsbGxT1xXQ0MDqqqqUFlZ2RHE1dXVGDNmjFcQK5VKDBw48Il/T3pyDF2iTjQ0NGDbtm0wm83IyMjAhg0bMGTIEACBO3uhtbUVtbW1HvvE58+fx/Xr1xEZGekVxmFhYaLVQr4xdIkecv/+fRQVFWHbtm1YsmQJ3njjDXzzm9/0uq59ylhWVhZWrVrlc4/X5XJh7969yM/Pl3XKWGNjI7788kuvLYoRI0Z4dVCoVCp+Pl5EDF2i/xEEAcePH0dGRgbGjx8Pk8mEqKioLu8pLy9HYWEhSkpKoNPpoNFoEBISArfbDYvFgsOHDyMhIQFpaWl+2VLwp7a2Nly+fNkriK9cuQK1Wu0VxmPHjpW75D6BoUsE4Msvv4TBYMC///1vvPPOO4iPj+9Rv2xdXV3HlyPcbjdCQkKgUqmQnJzc674ccevWLVRXV3u1swUHB3t1UEyZMgXBwcFyl+zlwS95NDU1QaFQQKVSBcSXPBi61K85nU5s3boVH3/8MbZs2YK1a9fyn9Y+CIKA//znPx4P7c6fP4/a2loolUqPFXF0dDTGjRsny0seFosFhYWFOHbsGJYuXerxL4+ysjIcPnwY8fHxSEtL8zkJTgoMXeqX7t69i8LCQuzYsQMrV67E66+/zodKj+HOnTuw2WweD+0qKyshCILXQ7vIyEgMHTpUtFp6y5c8GLrUrwiCgEOHDiErKwtRUVHIz8+HSqWSu6w+RRAEXL9+3auDwm63Y9KkSV5hPHHixCdeFQdqN4kvDF3qN8rLy5Geno7GxkaYTKZOZ96SOO7du4eamhqvveLbt28jKirK48Hd9OnToVAouvVzLRYLEhMTPfqm7969i3Xr1uHUqVMdbwG+/fbbWLhwYcd9/u6b7i6GLvV5V69exWuvvYZTp07hjTfewEsvvcQXBQJIXV2d16vPVqsVTz/9tMdDuxkzZuDZZ5/1evVZr9cjNjYWGzZs6Dh269Yt5OfnY9WqVZg4cSKOHz+OpKQkVFVVYdKkSR3XPc4bgk+KoUt9Vvv/eO+++y7WrFmDTZs2+RzLSIGnpaUFFy9e9Gpnc7lcmD59usfWRHJyMmpra7uchQEAM2bMQHZ2Nn74wx92HHvULAwxMHSpz2lra8OBAwewefNmzJ07F9u3b/dY3VDvdfPmTY9Xn0+ePIk5c+bg4MGDXd53/fp1PPPMM6ioqMCUKVM8zq1evRqRkZHYuHGjmKV34Dxd6lNKS0thMBgwcOBA/OEPf8CcOXPkLon8aNSoUdBqtdBqtQCA1NRUxMTEdHnP/fv3sWLFCqSkpHgFLvD1lzwqKirEKNcnhi71CbW1tTAajfj888+xfft2LF++nGMP+4HOvuTRrq2tDStXrsTgwYOxa9cun9e09/FKhX8rqVf76quvYDQaodFoEB0djQsXLuDFF19k4PYTnX3JA/i6de3ll1/G9evXcejQoU5feml/g1Aq/JtJvVJLSws++OADqNXqjqffW7ZswbBhw+QujSSkUqlQVlbm89zatWths9lw9OjRLl/KsFgskvZq80Ea9TonT56EwWDAmDFjYDKZ8O1vf1vukkgmTqcTarXaq3vh8uXLmDRpEoKDgzFo0P/vohYVFWHFihUd/83uBaIu2Gw2ZGRkwG63Iz8/H4sXL+ZHHMlnn253ydGny+0FCnj19fVYv3495s+fjwULFqC6uhpLlixh4BIAIC0tDXl5eXA4HD26z+FwYMeOHUhLSxOpMt8YuhSw7t27B5PJhKlTp0IQBNhsNhgMBgwePFju0iiAaDQa5ObmIi4urtvB2z57ITc3V/I5x2wZo4AjCAKOHDmCzMxMTJ48GadPn8a0adPkLosCWPvQGq1WG/Bf8uCeLgWUiooKGAwGOJ1O7Ny5E3FxcXKXRL1Ib/iSB0OXAsK1a9ewZcsWHDt2DDk5OfjpT3/q8dSZqCcC+UseDF2SVXNzM0wmEwoKCrB69Wps3rwZI0eOlLssItFwKUGyEAQBBw8exKZNm6DRaPD5558jIiJC7rKIRMfQJcl99tlnSE9PR0tLC/bv34/58+fLXRKRZNgyRpK5fPkykpKSsGzZMqxduxZlZWUMXOp3GLokOrfbjc2bNyMmJgZqtRo1NTVISUnhUBrql/i3nkTT2tqKPXv2QK1W48qVK6isrEROTg6GDx8ud2lEsuGeLonik08+gcFggEKhwJEjR6DRaOQuiSggMHTJr+x2OzIzM1FVVYW8vDz86Ec/4owEogdwe4H8oqGhAenp6fjud7+LuXPnwmq1YtmyZQxcoocwdOmJ3L9/H++++y7UajWam5thtVqRlZWFIUOGyF0aUUDi9gI9FkEQcPz4cWRkZGDChAn4y1/+gqioKLnLIgp4DF3qsaqqKhgMBly5cgU7d+7EokWLuI1A1E3cXqBuczqdWLNmDRYsWIDExERUVVUhPj6egUvUAwxdeqQ7d+4gLy8P06ZNw/Dhw1FTU4P169d3+nVVIuoctxeoU4Ig4I9//COMRiNmzJiBzz77DJMnT5a7LKJejaFLPlksFhgMBrjdbuzZswfPP/+83CUR9QncXiAPV69eRXJyMhYvXoxVq1bhiy++YOAS+RFXun2E0+nsmJTf1NQEhUIBlUqFlJSUbk3Kv3XrFnbs2IFdu3bhlVdeQU1NDUJCQiSonKh/4Uq3l7NYLNDr9VCr1bDZbIiJiUF8fDxiYmJgtVqhUqmg1+thsVh83t/W1oZ9+/ZBrVbDbrfj7Nmz+OUvf8nAJRKLQL1WUVGREB4eLhQUFAgul8vnNS6XSzCZTEJ4eLhQVFTkce706dPCzJkzhdmzZwtnzpyRomSifo/fSOuliouLkZeXhxMnTkCpVD7yeofDgbi4OBiNRixYsABZWVkoLy/H9u3bsXz5cvbaEkmEodsLWSwWJCYmorS0tFuB287hcGD27Nm4f/8+srKyYDAYMHToUBErJaKHcU+3FyosLITRaPQKXJfLBZ1Oh+HDh+OZZ57BRx995HFeqVRi06ZNeOGFF7B582YGLpEMuNLtZZxOJ9RqNWpraxEaGupxLikpCW1tbfjwww9RUVGB+Ph4nDlzBpGRkR3XuFwuREREwG63d6urgYj8iyvdXsZsNkOn03kF7q1bt3Do0CFs27YNCoUC8+bNQ2JiIvbv3+9xXVhYGHQ6Hcxms5RlE9H/MHR7GbvdjlmzZvk8PmjQIKhUqo5j0dHRqK6u9rpWo9HAbreLWicR+cbQ7WWampp89tA2NTVhxIgRHsdGjhwJt9vtdW1ISIjP40QkPoZuL6NQKHwGpkKhQGNjo8exxsZGnwHtdrv58gORTBi6vYxKpUJZWZnP4y0tLbh48WLHscrKSo+HaO0sFovHNgQRSYfdC71MV90L7S857NmzBxUVFVi0aBG7F4gCDFe6vcjt27fxwQcfQBAE/Pa3v/U6//7776O5uRljx45FUlISdu/e7bXS3bdvHxISEhi4RDLhSrcXaGtrw0cffYTXXnsNc+bMQVJSEtauXftYb6RptVocPXoUsbGxIlZMRJ3haMcA9+mnn8JgMAAADh48iLlz5wL4epshLi6ux7MXcnNzGbhEcpJp0A49Qm1trbBs2TJhwoQJwoEDB4TW1lava9qnjJlMpk6njN24cUPYuXOnzyljRCQ9bi8EmMbGRrz11lv49a9/jQ0bNmDjxo0YNmxYp9eXl5ejsLAQJSUl0Ol00Gg0HX24FosFhw8fRkJCAtLS0rjCJQoADN0A0dLSgg8//BA5OTlYuHAh3nzzTTz99NPdvr+urq7jyxHtfbgqlQrJycl8aEYUQBi6AeDkyZPYuHEjRo8eDZPJhJiYGLlLIiKR8EGajGw2GzIyMlBTU4P8/HwsWbKEw8SJ+jj26cqgvr4eP//5zzF//nwsWLAAVqsVOp2OgUvUDzB0JXTv3j2YTCZMnToVQUFBsNlsMBgMGDx4sNylEZFEuL0gAUEQ8Kc//QlZWVlQqVT4+9//jqlTp8pdFhHJgKErsnPnzsFgMKC+vh7vvfceXnjhBblLIiIZcXtBJP/973/x0ksvYeHChVi+fDnOnTvHwCUihq6/3b59G9u2bUNUVBTGjh2LmpoarFmzBoMG8R8VRMTtBb95cCjN7NmzUV5ejmeffVbusogowDB0/eAf//gHDAZDR/DOmzdP7pKIKEAxdJ/Av/71L7z66qs4c+YM3n77bbz44osYMIA7NkTUOSbEY2hsbMSrr76K2NhYREZGoqamBnq9noFLRI/ElOiBlpYWFBUVQa1W4/r16zh//jy2bt3a5RQwIqIHcXuhm/785z/DYDAgLCwMx44d41AaInosDN1HuHDhAjZu3MihNETkF9xe6MSNGzewfv16aLVaPP/886iuruZQGiJ6Ygzdh9y7dw8FBQWYMmUKBEGAzWbDxo0bERwcLHdpRNQHcHvhfwRBwJEjR5CZmYnJkyfj9OnTmDZtmtxlEVEfw9DF/w+lqaurw65duxAXFyd3SUTUR/Xr7YVr165h9erVWLhwIX7yk5+goqKCgUtEouqXodvc3Iw333wTUVFRGDNmDGpqavDKK69wKA0Ria5fpUxbWxsOHjyITZs24Tvf+Q7Kysrw3HPPyV0WEfUj/SZ0z5w5g/T0dA6lISJZ9fnQvXTpEoxGI86cOYO33noLK1as4IwEIpJNn02fxsZGbNq0CTNnzkRkZCQuXLiAlStXMnCJSFZ9LoFaW1tRXFwMtVqNa9eudQylGT58uNylERH1re2FU6dOwWAwIDQ0FCUlJZg5c6bcJREReQiI0HU6nTCbzbDb7WhqaoJCoYBKpUJKSgrGjBnzyPsvXLiAzMxMWK1W5Ofnc0YCEQUsWbcXLBYL9Ho91Go1bDYbYmJiEB8fj5iYGFitVqhUKuj1elgsFp/337hxA7/4xS8wb948fO9734PVasXSpUsZuEQUuASZFBUVCeHh4UJBQYHgcrl8XuNyuQSTySSEh4cLRUVFHcfv3r0rmEwm4amnnhLWrVsnOJ1OqcomInoismwvFBcXIy8vD6WlpVAqlZ1eFxoaivT0dPzgBz9AXFwcBEFAeHg4MjMzoVQqOZSGiHqdIEEQBCl/Q4vFgsTERK/A3bVrF/bu3YuqqiokJSVh7969Hvc5HA7MmjULo0aNwu7duzkjgYh6JclDV6/XIzY2Fhs2bPA4/vHHH2PAgAE4ceIEmpubvUIXAEwmE7744gv87ne/k6ZYIiI/kzR0nU4n1Go1amtrERoa6vOaLVu24OrVqz5D1+VyISIiAna7vVtdDUREgUbS7gWz2QydTtdp4D5KWFgYdDodzGaznysjIpKGpKFrt9sxa9asJ/oZGo0GdrvdTxUREUlL0tBtampCSEjIE/2MkJAQuN1uP1VERCQtSUNXoVA8cWC63e4nDm4iIrlIGroqlQplZWU+z7W0tODOnTtobW1Fa2sr7ty5g5aWFq/rLBYLVCqV2KUSEYkiYLoXcnJykJub63EsOzsbOTk5Hf/N7gUi6u0Cpk+3OwoKCnD27Fns37/f/4UREUkgYN5IexSHwwGtVoujR48iNjZWxAqJiMQj+ZQxjUaD3NxcxMXFweFwdOseh8OBuLg45ObmMnCJqFeTZbRjamoqjEYjtFotCgoK0NDQ4PM6l8sFk8kErVYLo9GI1NRUiSslIvIvybcXHlReXo7CwkKUlJRAp9NBo9F09OFaLBYcPnwYCQkJSEtL4wqXiPoEWUO3XV1dXceXI9r7cFUqFZKTk9mlQER9SkCELhFRf9HnvgZMRBTIGLpERBJi6BIRSYihS0QkIYYuEZGEGLpERBJi6BIRSYihS0QkIYYuEZGEGLpERBJi6BIRSYihS0QkIYYuEZGEGLpERBJi6BIRSej/AK6fl30trJWcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: currently expects data in array or list of tuples form\n",
    "model_cl = gm.fit_chowliu(np.array(D))\n",
    "\n",
    "fig,ax=plt.subplots(1,1); ax.set_axis_off();\n",
    "gm.drawMarkovGraph(model_cl,node_color='w');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pick any of these nodes as the root, and orient a Bayes net away from the root to form a tree. \n",
    "\n",
    "Since a more complex model can always fit the data better, Chow-Liu will inevitably choose a fully connected tree.\n",
    "However, `fit_chowliu` can add a BIC complexity penalty that allows it to select a forest, i.e., select fewer edges when they do not improve the log likelihood sufficiently.  Data can also be given weights, for example data from importance sampling, or weighted data during expectation-maximization learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> TODO: possible topics to add? </span>\n",
    "\n",
    "Fit methods\n",
    "* Chow Liu\n",
    "* BN Brute Force\n",
    "* ILP optimization\n",
    "* Group-L1-penalized PLL\n",
    "* Independence tests\n",
    "* Greedy?\n",
    "\n",
    "Refit methods\n",
    "* PLL (sgd; scipy.opt)\n",
    "* Gradient (estimator?)\n",
    "  * JT/MC/MCMC/Decomp/Var - work per gradient step?\n",
    "* IPF\n",
    "  * Infer method? VE/JT/etc\n",
    "  * \"full fit\" vs fixed point update (gradient like?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
